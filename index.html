<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="`FLARE` project page">
  <meta property="og:title" content="Learning Latent Prior for Rapid Adaptation of Legged Robots to Unexpected Amputation"/>
  <meta property="og:description" content="details about the paper `FLARE`"/>
  <meta property="og:url" content="https://hansooworld.github.io/flare/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="assets/images/fig_overview.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="assets/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Learning Latent Prior for Rapid Adaptation of Legged Robots to Unexpected Amputation</title>
  <link rel="icon" type="image/x-icon" href="assets/images/fire_logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="assets/css/bulma.min.css">
  <link rel="stylesheet" href="assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="assets/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="assets/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="assets/js/fontawesome.all.min.js"></script>
  <script src="assets/js/bulma-carousel.min.js"></script>
  <script src="assets/js/bulma-slider.min.js"></script>
  <script src="assets/js/index.js"></script>

  <!-- Include MathJax Library -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- model viewer -->
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>

  <!-- MathJax Configuration -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['$$', '$$']],
        processEscapes: true,
      },
      "HTML-CSS": { availableFonts: ["TeX"] },
      showMathMenu: false,
    });
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning Latent Prior for Rapid Adaptation of Legged Robots to Unexpected Amputation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/" target="_blank">Sunghyun Park</a><sup>1</sup>,  
                Yoonbyung Chai</<sup>1</sup>,  
                Seungyup Ka<sup>1</sup>,  
                Hyeonseong Kim<sup>1</sup>,  
                <br>
                Sangbeom Park</a><sup>1</sup>,
                Kevin Gim<sup>2</sup>,  
                Joohyung Kim<sup>2</sup>,  
                and Sungjoon Choi<sup>1</sup><sup>*</sup>
                <div class="is-size-5 publication-authors" style="text-align: center;">
                  <span class="author-university-block">Korea University<sup>1</sup><br></span>
                  <h2></h2>
                  <span class="author-block">University of Illinois Urbana-Champaign<sup>2</sup><br></span>
                </div>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
<hr>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <source src="assets/videos/FLARE_sup.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle" style="text-align: left;">
        <span style="font-weight: bold; font-size: larger">FLARE</span> (<b>F</b>ault-tolerant <b>L</b>earned <b>A</b>daptive <b>R</b>Einforcement learning)
        is an adaptive learning framework designed to efficiently learn locomotion policies that handle substantial hardware variations, such as leg amputations.
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Legged robots have shown remarkable progress in walking over challenging terrain. 
            However, they remain vulnerable to critical hardware failures due to unexpected accidents. 
            In this paper, we introduce a framework that enables rapid adaptation to unexpected configuration changes in legged robots. 
            To achieve this objective, we leverage a latent space to generate reusable prior knowledge by mapping capable walking trajectories (i.e., sequences of joint positions). 
            This learned prior guides exploration in new leg configurations, enabling rapid adaptation to unexpected changes with efficient locomotion trajectories. 
            We validate our framework by assessing whether previously learned walking strategies can be effectively applied to new configurations using a modular robot designed to operate in various leg configurations. 
            Our results show that the proposed method outperforms baseline approaches in terms of rapid adaptation across different leg configurations ranging from six to three legs in both simulation and real-world environments.          
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Problem Statement -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Statement</h2>
        <div class="content has-text-justified">
          <img src="assets/images/fig_problem_statement.png" alt="MY ALT TEXT"/>
          <p>
            Natural creatures know how to behave in the event of unexpected changes, such as limb amputation or sensory paralysis. 
            Some creatures, when faced with dangerous situations, can quickly escape after autotomy, which indicates the voluntary release of their limbs or tails. 
            For instance, Harvestman, an arthropod with eight legs, can learn to walk efficiently within a day after losing its legs by exploiting its prior knowledge of walking.         
            Motivated by nature, our work addresses the challenge of rapidly adapting and maintaining locomotion in the face of unpredictable circumstances.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Problem Statement -->

<!-- Framework Overview -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Framework</h2>
      <div id="framework">
        <!-- Content here -->
          <!-- Your image here -->
          <img src="assets/images/fig_framework_overview.png" alt="MY ALT TEXT"/>
          <h2 class="content has-text-justified" style="text-align: left;">
            An overall pipeline of the proposed adaptive learning framework.
            Our approach consists of two main phases: 
            (a) Learning a trajectory space to generate joint-specific locomotive knowledge (i.e., joint trajectory),
            and (b) sampling trajectories from the pre-trained decoder and then rearranging them based on leg indices <span style="background-color: #F8E1FF;">(shaded in purple)</span>.
            The rearranged trajectories allow an adaptation policy to generate joint invariant trajectories, enabling adapted locomotion across different leg configurations <span style="background-color: #FFFF99;">(shaded in yellow)</span>.
          </h2>
      </div>
    </div>
  </div>
</section>
<!-- End of Framework Overview -->

<!-- Simulation Experiments Videos -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Simulation Experiments</h2>
      <div id="module">
        <div id="sim-results">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="assets/videos/video_4.mp4" type="video/mp4">
          </video>
          <h2 class="content has-text-justified" style="text-align: left;">
            <!-- User interaction with the proposed system SPOTS. The user selects among the candidates, provided with a close consideration of stability and reasonableness, in an interactive viewer. SPOTS recommends the placement candidates based on the prompt of the task. -->
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End of Simulation Experiments Videos -->

<!-- Real-world Experiments Videos -->
 <section class="hero is-small"></section>
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Real-world Experiments</h2>
      <div id="module">
        <div id="real-results">
          <div class="columns is-variable is-8 is-multiline">
            <div class="column is-half">
              <video poster="" id="video1" autoplay controls muted loop width="100%">
                <source src="assets/videos/video_7.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <!-- 두 번째 비디오 -->
            <div class="column is-half">
              <video poster="" id="video2" autoplay controls muted loop width="100%">
                <source src="assets/videos/video_8.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <!-- <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="assets/videos/video_7.mp4" type="video/mp4">
          </video>
          <h2 class="content has-text-justified" style="text-align: left;">
            User interaction with the proposed system SPOTS. The user selects among the candidates, provided with a close consideration of stability and reasonableness, in an interactive viewer. SPOTS recommends the placement candidates based on the prompt of the task.
          </h2> -->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End of Real-world Experiments Videos -->

<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{lee2023spots,
          title={SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems},
          author={Lee, Joonhyung and Park, Sangbeom and Park, Jeongeun and Lee, Kyungjae and Choi, Sungjoon},
          journal={arXiv preprint arXiv:2309.13937},
          year={2023}
        }
      </code></pre>
    </div>
</section> -->
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>

<script>

  timeoutIds = [];

  function populateDemo(imgs, num) {
      // Get the expanded image
      var expandImg = document.getElementById("expandedImg-" + num);
      // Get the image text
      var imgText = document.getElementById("imgtext-" + num);
      var answer = document.getElementById("answer-" + num);

      // Use the same src in the expanded image as the image being clicked on from the grid
      expandImg.src = imgs.src.replace(".png", ".mp4");
      var video = document.getElementById('demo-video-' + num);
      // or video = $('.video-selector')[0];
      video.pause()
      video.load();
      video.play();
      video.removeAttribute('controls');

      console.log(expandImg.src);
      // Use the value of the alt attribute of the clickable image as text inside the expanded image
      var qa = imgs.alt.split("[sep]");
      imgText.innerHTML = qa[0];
      answer.innerHTML = "";
      // Show the container element (hidden with CSS)
      expandImg.parentElement.style.display = "block";
      for (timeoutId of timeoutIds) {
          clearTimeout(timeoutId);
      }

      // NOTE (wliang): Modified from original to read from file instead
      fetch(qa[1])
          .then(response => response.text())
          .then(contents => {
              // Call the processData function and pass the contents as an argument
              typeWriter(contents, 0, qa[0], num);
          })
          .catch(error => console.error('Error reading file:', error));
  }

  function typeWriter(txt, i, q, num) {
      var imgText = document.getElementById("imgtext-" + num);
      var answer = document.getElementById("answer-" + num);
      if (imgText.innerHTML == q) {
          for (let k = 0; k < 5; k++) {
              if (i < txt.length) {
                  if (txt.charAt(i) == "\\") {
                      answer.innerHTML += "\n";
                      i += 1;
                  } else {
                      answer.innerHTML += txt.charAt(i);
                  }
                  i++;
              }
          }
          hljs.highlightAll();
          timeoutIds.push(setTimeout(typeWriter, 1, txt, i, q, num));
      }
  }

</script>
</html>